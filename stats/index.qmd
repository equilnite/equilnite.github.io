---
title: "AP Stat Notation/Formula cheatsheet"
author: "Edward Chang `edwardchang@berkeley.net`"
institute: "Berkeley High School"
format: html
table-of-contents: true
toc-depth: 4
editor: 
  markdown: 
    wrap: 72
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, fig.height = 7)
```

```{r, message = FALSE, warning = FALSE}
table_show = rmarkdown::paged_table
library(tidyverse)
library(knitr)
```

## Basic notation

### Lists

Most of the time, you will see $x$ denote a list of values (i.e. a
variable in a data table).

For example, if $x$ was the list of numerical values $a$ to $g$, we can
write it as:

$x = [a, b, c, d, e, f, g]$

Then, $x_i$ means the $i^{th}$ value in the list of $x$, so

$x_1 = a$ and $x_2 = b$ and $x_7 = g$.

------------------------------------------------------------------------

### $n$

In regards to a data table or list of values, $n$ stands for the number
of rows or data points that are in the data table or list (we will learn
this as the *sample size* later on)

So, for list $x$, $n = 7$.

Adding on, $x_n$ would mean the last value in the list $x$ (since there
are only $n$ values in $x$)

------------------------------------------------------------------------

### Summation ($\Sigma$)

You will also see the greek letter $\Sigma$ in formulas. Usage of this
sign means that we are using *summation notation*.

If we want the sum of all numbers from 1 to 7, we would write it as,

$$\sum_{i=1}^7 i$$

We interpret this as,

1.  Start from $i = 1$, evaluate the expression, which is $i$.

2.  Keep our evaluated expression to the side and get ready to add the
    other values to it, so

$$1 + \cdots$$

3.  Now go the next numbers until we get to $7$ (the number on the top
    of the $\Sigma$) So moving onto $i = 2$, we end up with
    $$1 + 2 + \cdots$$ \newline And with $i = 3$, we end up with
    $$1 + 2 + 3 + \cdots$$ \newline

4.  When we get to the end of it (when we reach $i = 7$), we have the
    *expanded form* the summation. $$1 + 2 + 3+ 4 + 5 + 6 + 7$$

------------------------------------------------------------------------

### Other notation

-   $\bar x$: The "line" on top of $x$ means the *mean of* $x$. If we
    had $\bar a$, I would be asking for the mean of $a$.
    -   Pronounced "x bar"
-   $\hat p$: The "hat" on top of $p$ means the *estimate of* $p$. If we
    had $\hat x$, I would be asking for the estimate of $x$.
    -   Pronounced "p hat"

------------------------------------------------------------------------

### Other commonly used symbols

-   $p$: proportion, probability, or p-value

-   $\bar x$: sample mean

-   $S_x$: sample standard deviation (of x), so $S_y$ is the sample
    standard deviation of $y$

-   $\mu$: population mean (true mean)

-   $\sigma$: population standard deviation (true standard deviation)

-   $N$: population size

------------------------------------------------------------------------

## Descriptive Statistics (WIP)

These formulas are all included on the [AP Cheatsheet included for your
use during the AP test, quizzes, and tests in this
class](https://apcentral.collegeboard.org/pdf/statistics-formula-sheet-and-tables-2020.pdf)

### Mean (Average)

$$\bar x = \frac{1}{n} \sum x_i = \frac{\sum x_i}{n}$$

### Sample standard deviation

$$S_x = \sqrt{\frac{1}{n-1}\sum(x_i - \bar x)^2} = \sqrt{\frac{\sum(x_i - \bar x)^2}{n-1}}$$

### Linear regression

Estimate: $$\hat y = a + bx$$

Linear regression passes through the mean: $$\bar y = a + b \bar x$$

\*Note: $a$ is the intercept of the line and $b$ is the slope of the
line

$$b = r \frac{S_y}{S_x}$$

### Correlation coefficient $r$

$$r = \frac{1}{n-1}\sum\left(\frac{x_i - \bar x}{S_x}\right)\left(\frac{y_i - \bar y}{S_y}\right)$$

## Categorical Data Visualizations

### Bar plots {#bar-plots}

Represent the number or proportion of each unique value. These numbers
or proportions are represented with rectangular bars with heights
proportional to the values that they represent. You can plot these
vertically or horizontally (i.e. categories on the x-axis or categories
on the y-axis)

Following data from this table:

```{r }
set.seed(123)
i <- sample(1:nrow(iris), 10, repl = FALSE)
iris_cat_tab <- iris %>% slice(i) %>% select(Species)
table_show(iris_cat_tab)
```

1.  Count up the number of values per category (make a frequency table).
    Note: This table is missing the total

```{r}
table_show(as.data.frame(table(iris_cat_tab), col.names = c("Species", "Frequency")))
```

2.  Plot the frequencies with them as the height of the bars

```{r}
ggplot(iris_cat_tab, aes(x = Species)) +
    geom_bar() +
    labs(title = "Bar plot of Iris Species")
```

If needed (if you need proportions for the y-axis instead, calculate the
relative frequency table for the frequency table first). Note: again,
this one is missing the total

```{r}
table_show(as.data.frame(prop.table(table(iris_cat_tab)), col.names = c("Species", "Frequency")))
```

```{r}
ggplot(iris_cat_tab, aes(x = Species, y = ..prop.., group = 1)) +
    geom_bar() +
    labs(title = "Bar plot of Iris Species", y = "Proportion")
```

### Stacked Bar Plots and Side-by-Side Bar Graphs

Stacked bar plots show two categorical variables, one on the
x-axis/y-axis, and the other as the legend (colours). We will call the
variable on the x-axis as the "groups" and the variable on the legend as
the "categories."

When constructing these bar plots, we first want to determine which
variable goes where (your choice or given choice to you). Then you
calculate relative frequencies *per group*

For example, here I have a two-way table detailing the hair and eye
colour of some statistics students

```{r}
library(reshape)
hair <- HairEyeColor[,,2] + HairEyeColor[,,1]
hair_table <- untable(hair)
table_show(as.data.frame(hair))
```

So if I want eye colour to be my groups, I would calculate the relative
frequencies by column (use the total of the column and divide the whole
column by it), so each group/column will add up to 1.

```{r}
table_show(as.data.frame(prop.table(hair, 2)))
```

These numbers will be my bar heights. So for the bar(s) representing
brown eyes:

-   black hair will be .3091

-   brown hair will be .5409

-   red hair will be 0.1182

-   blond hair will be 0.0318

```{r}
barplot(prop.table(hair, 2), legend = TRUE, xlab = "Eye Color", ylab = "Proportion", main = "Segmented Barplot of Hair Color on Eye Color")
```

Here's the corresponding side-by-side bar plot. Note that the heights of
the bars are the same as the segmented bar graph.

```{r}
barplot(prop.table(hair, 2), legend = TRUE, xlab = "Eye Color", ylab = "Proportion", main = "Side-by-side Barplot of Hair Color on Eye Color", beside = TRUE)
```

On the other hand, if I want my eye colour to be my groups, I would
calculate the relative frequencies by row (use the total of the row and
divide the whole row by it), so each group/row will add up to 1.

```{r}
table_show(as.data.frame(prop.table(hair, 1)))
```

These numbers will be my bar heights. So for the bar(s) representing
black hair:

-   brown eyes will be 0.6296296

-   blue eyes will be 0.1851852

-   hazel eyes will be 0.1388889

-   green eyes will be 0.0462963

```{r}
barplot(t(prop.table(hair, 1)), legend = TRUE, xlab = "Hair Colour", ylab = "Proportion", main = "Segmented Bar plot of Eye Colour on Hair Colour")
```

```{r}
barplot(t(prop.table(hair, 1)), legend = TRUE, xlab = "Hair Colour", ylab = "Proportion", main = "Side-by-side Bar plot of Eye Colour on Hair Colour", beside = TRUE)
```

### Mosaic Plots

Mosaic plots are the almost the same as stacked bar plots. The only
difference is that the widths of the bars change according to the
proportion of points in each group. In a mosaic plot, the x-axis will
also measure the proportion of observations/data points within the
groupings (i.e. the x-axis reflects the marginal distribution of the
variable on the x-axis).

Following the [same
steps](#stacked-bar-plots-and-side-by-side-bar-plots) as the
side-by-side and stacked bar charts to find the heights, we now add an
additional step before plotting.

*Find the widths of the bars by finding the marginal distriubtion of the
variable on the x-axis (the groups)*

1.  For each group, find the probability of having that trait. So for
    our previous example, we had this table:

```{r}
table_show(as.data.frame(hair))
```

Using our eye colours as the groups (vertical bars), we will find:

-   $P(Brown) \approx .3716$

-   $P(Blue) \approx .3632$

-   $P(Hazel) \approx .1571$

-   $P(Green) \approx .1081$

When we plot our mosaic plot, we do the same thing, except now, we have
our bars differ in widths according to the numbers that we just
calculated.

![Mosaic Plot of Hair Colour on Eye Colour](moplotex.png)

## Quantative Data Visualizations

### Dot Plots

Dot plots are for discrete quantitative variables only, and they are
only useful in situations when you have a small range of number so that
you can actually see how the data distribution varies across values.

Dot plots are simple, you draw a number line and then plot points above
the number for each of the number that you see in the data.

Take this data for example:

```{r}
set.seed(123)
X <- data.frame(`Number of Holligans in the Hallway` = round(rnorm(20, 5, 2)))
table_show(X)
```

Now count up each value to figure out how many dots you need at each
value on the number line then plot your graph

```{r}
table_show(as_tibble(table(X), colnames = c("Number of People", "Frequency")))
ggplot(X, aes(x = Number.of.Holligans.in.the.Hallway, y = ..count..)) + geom_dotplot() +
    scale_x_continuous(breaks = 1:9, labels = 1:9) +
    labs(y = "Frequency", title = "Dot Plot of the Number of People in the Hallway")
```

### Stemplots

Using this data as an example, a stem plot looks like this:

```{r}
set.seed(123)
x <- round(rnorm(20, 50, 25))
table_show(data.frame(x))
```

A stem plot looks like this:

```{r}
#install.packages("aplpack")
library(aplpack)
stem.leaf(x, style = "bare", depths = FALSE)
```

In a stem plot, you need to determine a common "stem" of all the numbers
that you're plotting. So if you have integer numbers from 10 to 200,
your stems will be everything from the tens and so on, so you'll have
stems from 1-20. Once you take the stems, you just write the "leaves"
next to the stem that they belong.

Note that you also have to add a key to show what a stem + leaf means.
The stem and leaves give no information on the decimals in the data, so
as you see above, you need to give an example like (as shown in the
example stemplot):

Key: 1\|2 = 12

Here's another example (sorted for convenience)

```{r}
x <- round(rnorm(50, 5, 1), digits = 1)
table_show(data.frame(x))
stem.leaf(x, style = "bare", depths = FALSE)
```

### Boxplots

*Also known as a box-and-whisker plot*

Boxplots are primarily made of the **five number summary** of the data.
The five number summary is made up of the:

-   Minimum (min)

-   First Quartile ($Q_1$)

-   Median

-   Third Quartile ($Q_3$)

-   Maximum (max)

To make a simple boxplot, you use the first quartile, median, and third
quartile to make the "box" and then use the minimum and maximum to make
the "whiskers."

For this simple list of numbers:

```{r}
x = c(1,2,3,4,5,6,7)
table_show(data.frame(x))

```

Our five number summary is:

```{r}
summary(x, quantile.type = 1)[-4]
```

As detailed above, our box plot then looks like:

```{r}
library(qboxplot)
qboxplot(data.frame(x), qtype = 1, horizontal = TRUE)
```

The last detail is that we can calculate outliers using the 1.5 IQR rule
and show them on the boxplot. For either direction (left or right), if
we see outliers in that direction, we only extend the whisker to the
smallest and/or largest point that is not an outlier. Then we plot any
outliers as individual points.

Look at this example data:

```{r}
set.seed(1234)
x <- round(rnorm(40, 0, 5))
table_show(arrange(data.frame(x), desc(x)))
```

Five number summary:

```{r}
summary(x, quantile.type = 1)[-4]
```

Our numbers calculated by the 1.5 IQR rule are:

```{r}
c(-5, 0) + 1.5 * c(-5, 5)
```

So our 12 is an outlier. which means we draw our right whisker to 6 and
plot the 12 individually on the number line. Like so:

```{r}
qboxplot(data.frame(x), qtype = 1, horizontal = TRUE)
```

### Histograms

A histogram is similar to a [bar plot](#bar-plots), except that
histograms are made for quantitative data and *bars are continuous* in
the sense that there is no gap between bars. To make a histogram, select
an appropriate equal intervals that make it so that you don't have too
many bars and that you don't have too few bars. Your goal with
histograms, as with many other visualizations, is to be able to see the
shape and characteristics of the distribution in question. If you have
too many bars or too few bars, you won't be able to see much important
information (especially think of situations when you have many data
points with very precise decimal measurements).

1.  Decide on your intervals (e.g. by 5's, by 10's, by 100's)

2.  Within your intervals, count up the number of observations that
    belong in that "bin". When you do so, count up observations so that
    you count the left end inclusive and the right end inclusive. So if
    you did intervals of 5, you would do something like counting up
    points $0 \leq x < 5$, $5 \leq x < 10$, and so on.

3.  Plot your bars.

Example:

Consider this example data set:

```{r}
ex_data <- data.frame(x = log(rnorm(20, 10, 3)))
```

Our data has this set of summary statistics:

```{r}
summary(ex_data)
```

With this knowledge, let's make our 7 "bins", so let's do these by every
0.2, starting at 1.5 to 2.9. This will be something that you build by
intuition.

Now, count up our values:

```{r}
summary(cut(ex_data$x, breaks = seq(1.5, 2.9, 0.2), include.lowest = TRUE, right = FALSE))
```

Now, we just put it together. For each bin, we have a bar and the bars'
heights correspond to the number of individuals in each bin.

```{r}
ggplot(ex_data, aes(x = x)) + geom_histogram(breaks = seq(1.5, 2.9, 0.2), color = "blue", alpha = 0.5) + scale_x_continuous(breaks = seq(1.5, 2.9, 0.2))
```

Again, just like bar graphs, we can instead do the relative frequencies
(this is what you'll see most of the time!!!)

```{r}
prop.table(summary(cut(ex_data$x, breaks = seq(1.5, 2.9, 0.2), include.lowest = TRUE, right = FALSE)))
```

```{r}
ggplot(ex_data, aes(x = x, y = ..density..)) + geom_histogram(breaks = seq(1.5, 2.9, 0.2), color = "blue", alpha = 0.5) + scale_x_continuous(breaks = seq(1.5, 2.9, 0.2))
```

When you have a histogram like this, keep in mind that the bars always
add up to 1 (or 100%).

------------------------------------------------------------------------

# Sampling

When we have a question about a certain population (an entire group of
individuals). Ideally we would ask them all (take a **census**). But
contacting every member of a population often isn't very practical: it
would take too much time and cost too much money. Instead, we put the
question to a **sample**, or subset of individuals of the population
from which we actually collect data, chosen to represent the entire
population.

When you want to identify the population, ask yourself, what does the
question want to know about? What group of people does the
question/problem address?

When identifying the sample, ask yourself, what group does the work done
actually address?

## Bias

When we collect data, there is the possibility of the data becoming
systematically pushed towards a specific outcome. For example, if we
want to learn about the GPA average in the school and take a sample of
students only from a class, it's quite possible that the sample is not
representative of the school. We will probably result in a GPA average
that is higher or lower than the actual GPA average in the school. There
are several ways that this can happen. The main way that we learn are:

### Response Bias

### Non-response Bias

### Voluntary Bias

# Probability

A process has a valid probability model if and only if:

-   Each outcome has a positive probability.
-   The sum of all outcomes' probabilities is equal to 1.

# Random Variables

A **random variable** is a variable whose value is a numerical outcome
of a random phenomenon.

Random variables can be discrete or continuous. A **discrete random
variable** $X$ has a countable, or smaller, number of possible outcomes,
while a **continuous random variable** $X$ can take on an infinite
number (theoretically) of different values.

The **probability distribution** of a random variable $X$ gives us all
possible values of $X$, and their corresponding probabilities.
Probability distributions are typically given as tables, histograms
(with probability on the y-axis, instead of frequency), or density
curves (like the Normal curve).

## Discrete Random Variables

A discrete random variable describes a process that only has specific,
predefined outcomes. For example, you can be finding the probability of
people having blue, brown, and black eyes. Or you can be finding the
probability of people earning salaries in the ranges of \<\$30,000,
\$30,000 - \$50,000, \$50,000 - \$ 70,000, \$70,000 - \$ 100,000,
\>\$100,000.

When we have a discrete random variable $X$ whose probability
distribution is

$$
\begin{align*}
    \textbf{Value:}& ~~~~ x_1 ~~~~ x_2 ~~~~ x_3 ~~~~ \cdots \\
    \textbf{Probability:}& ~~~~ p_1 ~~~~ p_2 ~~~~ p_3 ~~~~\cdots
\end{align*}
$$

we know the following about the mean and standard deviation of $X$:

$$\mu_X = E(X) = \sum x_i P(x_i)$$

$$\sigma_X = \sqrt{\sum \left( x_i - \mu_X \right) ^2 P \left( x_i \right)}$$

<details>

<summary>Keep in mind that the standard deviation formula here (and else
where) is equivalent to the formula of population standard deviation
when we divide by $n$ instead of multiplying by $P(x_i)$.</summary>

$$
\begin{aligned}
\sigma_X &= \sqrt{\frac{\sum(x_i - \mu_X)^2}{n}} \\
&=\sqrt{E((X - \bar x)^2)} \\
&=\sqrt{\sum \left( x_i - \mu_X \right) ^2 P \left( x_i \right)}
\end{aligned}
$$

In other words, remember that we define standard deviation as the root
mean square of the squared differences from the mean. Since we are
taking the mean of the squared differences from the mean in both
formulas (I use the definition for mean from step 1 to 2 and the
definition of mean for discrete variables from step 2 to 3), the two
formulas are equivalent.

</details>

The variance of a random variable $X$ is:

$$
Var(X) = \sigma_X^2
$$

### Binomial Random Variables

Binomial random variables have parameters $n$ and $p$, and can be
written $B(n, p)$. Remember, Normal random variables have parameters and
and can be written $N(\mu,\sigma)$.

The pdf of a Binomial Random Variable (i.e. the binomial formula) is: $$
\begin{align*}
    P(X=k) &= {n \choose k} p^k (1-p)^{n-k}\\
    \text{where } k &= 0, 1, 2, 3, \cdots, n
\end{align*}
$$

To apply this formula in a graphing calculator:
$\texttt{2nd -> vars (distr) -> binompdf}$

Usage: $\texttt{binompdf(n, p[,x])}$

The cdf of a Binomial Random Variable is: $$
\begin{align*}
    P(X\leq k) &= \sum_{i = 0}^n {n \choose i} p^i (1-p)^{n-i}
\end{align*}
$$

In graphing calculator: $\texttt{2nd -> vars (distr) -> binomcdf}$

Usage: $\texttt{binomcdf(n, p[,x])}$

The mean and standard deviation of a binomial random variable is given
by:

$$
\begin{align*}
    \mu_X &= n p \\
    \sigma_X &= \sqrt{n p q} \\
    &\text{, where } q = 1-p.
\end{align*}
$$

#### Binomial setting

We can identify a binomial setting when we know:

1.  **B**inary? The possible outcomes of each trial can be classified as
    "success" or "failure" (in our case rolling a 7 or not).

2.  **I**ndependent? Trials must be independent; that is, knowing the
    result of one trial must not tells us anything about the result of
    another trial.

3.  **N**umber? The number of trials n of the chance process must be
    fixed in advance. (this was 5, then 100).

4.  **S**ame? There is the same probability p of success on each trial
    (1/6).

#### 10% Condition

The second condition is often not perfectly met, as in the case of an
SRS from some population. Imagine choosing 10 students from a class of
15 females and 15 males---as we choose people, the remaining population
changes, which changes the probability that the next person chosen will
be male or female.

When we lack complete independence, we can see the consequence of this
is negligible as long as our sample is small relative to the population
from which we are sampling. If we were choosing our 10 people from a
school of 3,300 students, the change in probability from person to
person would be small enough to ignore.

The general rule is that the sample needs to be less than
$\frac{1}{10}$, or 10%, of the population. We refer to this as the 10%
condition.

$$n \leq (.10) N$$

#### Normal Approximation to the Binomial Distribution

Remember that as $n$ gets large, a binomial random variable $X$ can take
on more and more different values, and it can become tedious to continue
to treat X as a discrete random variable. As $n$ get larger, we can
treat $X$ as a continuous random variable, more specifically:

As $n$ gets larger, the binomial distribution gets closer to a normal
distribution. However, before we use a normal distribution to
approximate a binomial distribution, we have to check the following
condition:

##### Large Counts condition

If $np \geq 10$ and $n(1-p) \geq 10$, then we can use a Normal
distribution to model a binomial distribution. In other words, if the
expected number of successes and failures (respectively) is greater than
or equal to 10.

##### Doing a normal approximation

First verify all the conditions for a Binomial setting and the Large
Counts Condition. Since we know that we have a binomial setting, we then
know the distribution that we want to use is $Normal(np, \sqrt{npq})$ proceed with the calculations according to this distribution.

### Geometric Random Variables

If $X \sim G(p)$, in other words, if $X$ has a geometric distribution
with parameter $p$, the pdf of a geometric random variable is:

$$
\begin{align*}
    P(X=x) &= (1-p)^{x-1}p\\
    \text{where } x &= 1, 2, 3, \cdots
\end{align*}
$$

In graphing calculator: $\texttt{2nd -> vars (distr) -> geometpdf}$

Usage: $\texttt{geometpdf(p, x)}$

The cdf of a Geometric Variable is:

$$
\begin{align*}
    P(X\leq x) &= \sum_{i=1}^x(1-p)^{i-1}p
\end{align*}
$$

In graphing calculator: $\texttt{2nd -> vars (distr) -> geometcdf}$

Usage: $\texttt{geometcdf(p, x)}$

The mean and standard deviation of a geometric random variable is given
by:

$$
\begin{align*}
    \mu_X &= \frac{1}{p} \\
    \sigma_X &= \frac{\sqrt{q}}{p} \\
    &\text{, where } q = 1-p.
\end{align*}
$$

#### The Geometric Setting

A geometric setting is very similar to a binomial setting, except that
\textbf{n, the number of trials is not fixed}.

A geometric setting is defined as a series of observations where these 4
conditions are met:

1.  **B**inary? The Possible outcomes of each trial can be classified as
    "success" or "failure"

2.  **I**ndependent? Trials must be independent, that is, knowing the
    result of one trial must not have any effect on the result of any
    other trial.

3.  **T**rials? The goal is to count the number of trials until the
    first success occurs.

4.  **S**uccess? On each trial, the probability p of success must be the
    same.

## Operations with Random Variables

### Constants

When we add a constant $a$ and/or multiply by a constant $b$ to a random
variable $X$, we perform a linear transformation of the form

$$
a + bX
$$

The mean of the transformed variable is:

$$
\mu_{a+bX}=a+\mu_{bX}=a+b\mu_X
$$

The standard deviation of the transformed variable is:

$$
\sigma_{a+bX}=\sigma_{bX}=b\sigma_X
$$

### Random Variables

In general, we can describe the mean and standard deviation of the sum
or difference of independent random variables with these formulas:

\$\$\\mu\_{X \\pm Y} = \\mu_X \\pm \\mu_Y\$\$

**If the random variables \$X\$ and \$Y\$ are independent**, then

\$\$ \\sigma\_{X \\pm Y}\^2 = \\sigma_X\^2 + \\sigma_Y\^2 \$\$
